{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ1Fc4GeZ7XEEqDlzH7Adp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabriellybc/previsao-evasao-ufes/blob/main/previsao_evasao_ufes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsX0NAuvkNJN"
      },
      "outputs": [],
      "source": [
        "#@title # Previsão de Alunos com Risco de Evasão - UFES { display-mode: \"form\" }\n",
        "#@markdown Este notebook utiliza um modelo de aprendizado de máquina **XGBoost** para prever alunos com risco de evasão da Universidade Federal do Espírito Santo (UFES), a partir de dados do histórico acadêmico importados.\n",
        "#@markdown ## Como funciona este notebook\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown 1. **Importação dos dados**\n",
        "\n",
        "#@markdown Ao executar a célula para carregar os dados, aparecerá um botão **\"Escolher arquivos\"** para que você selecione as bases de histórico dos alunos.\n",
        "#@markdown - Você pode selecionar quantos arquivos quiser.\n",
        "#@markdown - **Importante:** Os arquivos devem conter obrigatoriamente as seguintes colunas: `MATR_ALUNO`, `ANO_INGRESSO`, `FORMA_INGRESSO`, `COTISTA`, `CRA`, `COD_FORMA_EVASAO`, `SITUACAO_ITEM`, `ANO`, `ANO_EVASAO`, `ANO_INGRESSO`\n",
        "\n",
        "#@markdown 2. **Pré-processamento dos dados**\n",
        "\n",
        "#@markdown Após a importação, os dados serão pré-processados para ficarem no formato adequado para a previsão pelo modelo.\n",
        "#@markdown O pré-processamento inclui a criação das seguintes colunas que serão usadas na previsão: `TEMPO_GRADUACAO`, `REPROVADA_POR_CURSADA_ANO1`, `REPROVOU_Nunca`,`REPROVOU_FREQUENCIA_Nunca`\n",
        "\n",
        "#@markdown 3. **Colunas referentes às disciplinas do departamento**\n",
        "\n",
        "#@markdown O processo é o seguinte:\n",
        "#@markdown - Para cada linha, identifica-se o departamento da disciplina.\n",
        "#@markdown - Os dados são agrupados e ordenados para identificar os **4 departamentos com maior número de registros** na base.\n",
        "#@markdown - Apenas esses 4 departamentos são selecionados; os demais são ignorados.\n",
        "#@markdown - Para cada aluno, verifica-se a porcentagem de valores nulos para esses departamentos (valores nulos indicam que o aluno nunca cursou disciplinas daquele departamento).\n",
        "#@markdown - São selecionados apenas os departamentos cuja porcentagem de nulos está abaixo de um limite (padrão: 1.6%).\n",
        "#@markdown - Ao final, o notebook imprime quais departamentos foram selecionados ou informa se nenhum departamento foi selecionado.\n",
        "#@markdown - Além disso, são removitos todas as linhas com alunos que tiverem valor nulo em alguma dessas colunas usadas na previsão.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## Parâmetros de entrada\n",
        "\n",
        "#@markdown Você deve definir os seguintes parâmetros para rodar o notebook:\n",
        "\n",
        "#@markdown - **URL_PASTA_DRIVE**: Link da pasta do Google Drive onde o modelo (`model.pkl`) e o arquivo de configuração das colunas (`config_features.json`) serão salvos ou lidos.\n",
        "#@markdown  - No modo `treinar_e_prever`, este campo é opcional. Se preenchido, o modelo treinado e as colunas selecionadas serão salvos nessa pasta. Se deixado vazio, o modelo e as colunas não serão salvos.\n",
        "#@markdown  - No modo `apenas_prever`, este campo é obrigatório e deve conter uma pasta com os arquivos `model.pkl` e `config_features.json` para que o notebook possa carregar o modelo e as colunas.\n",
        "\n",
        "#@markdown - **MODO**: Define se o notebook vai:\n",
        "#@markdown  - `treinar_e_prever`: Treinar um novo modelo com os dados importados e fazer a previsão.\n",
        "#@markdown  - `apenas_prever`: Apenas fazer a previsão usando um modelo previamente treinado e salvo na pasta do Drive.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## Observações importantes\n",
        "\n",
        "#@markdown - Cada vez que você rodar o modo `treinar_e_prever` com o campo `URL_PASTA_DRIVE` preenchido, os `arquivos model.pkl` e `config_features.json` serão salvos na pasta indicada.\n",
        "#@markdown - Na pasta do Drive, podem existir múltiplos arquivos com esses nomes, referentes a diferentes execuções.\n",
        "#@markdown - No modo `apenas_prever`, o notebook irá carregar os arquivos `model.pkl` e `config_features.json` com a última data de modificação mais recente para garantir que o modelo mais atualizado seja usado.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## Entradas:\n",
        "\n",
        "URL_PASTA_DRIVE = \"https://drive.google.com/drive/folders/12R6swOXmMTAGJsMEHfkJlfIoAxvxezQ2\" #@param {type:\"string\"}\n",
        "MODO = \"treinar_e_prever\" # @param [\"treinar_e_prever\",\"apenas_prever\"]\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive, auth, files\n",
        "import google.auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Any\n",
        "import io\n",
        "import gc\n",
        "import json\n",
        "import pickle\n",
        "from mimetypes import guess_type\n",
        "from collections import Counter\n",
        "from scipy import stats\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_validate\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 999)\n",
        "credentials, project = google.auth.default(\n",
        "    scopes=[\"https://www.googleapis.com/auth/drive\"]\n",
        ")\n",
        "service_drive = build(\"drive\", \"v3\", credentials=credentials)\n",
        "\n",
        "\n",
        "MAIN_COLUMNS = [\n",
        "    \"MATR_ALUNO\",\n",
        "    \"ANO_INGRESSO\",\n",
        "    \"FORMA_INGRESSO\",\n",
        "    \"COTISTA\",\n",
        "]\n",
        "COL_X = [\n",
        "    \"CRA\",\n",
        "    \"TEMPO_GRADUACAO\",\n",
        "    \"REPROVADA_POR_CURSADA_ANO1\",\n",
        "    \"REPROVOU_Nunca\",\n",
        "    \"REPROVOU_FREQUENCIA_Nunca\"\n",
        "]\n",
        "col_y = \"GRUP_FORMA_EVASAO_Evadido\"\n",
        "\n",
        "MODEL_NAME = \"model.pkl\"\n",
        "JSON_NAME = \"config_features.json\"\n",
        "QNT_MAX_DEPARTMENTS = 4\n",
        "MAX_NULL_PERCENT = 1.6\n",
        "OUTPUT_FILENAME = \"previsao_evasao_estudantes.xlsx\"\n",
        "\n",
        "# Data Loading\n",
        "def upload_files() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Uploads and combines multiple CSV and Excel files into a single pandas DataFrame.\n",
        "    \"\"\"\n",
        "    list_of_dfs = []\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"Nenhum arquivo foi carregado.\")\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith(\".csv\"):\n",
        "            df = pd.read_csv(filename)\n",
        "        elif filename.endswith(\".xlsx\"):\n",
        "            df = pd.read_excel(filename)\n",
        "        list_of_dfs.append(df)\n",
        "    if not list_of_dfs:\n",
        "        raise ValueError(\"Nenhum arquivo válido foi carregado.\")\n",
        "    combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
        "    del list_of_dfs\n",
        "    del uploaded\n",
        "    gc.collect()\n",
        "    return combined_df\n",
        "\n",
        "\n",
        "def convert_types(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Converts the data types of specific columns in the DataFrame.\n",
        "    \"\"\"\n",
        "    df[\"CRA\"] = pd.to_numeric(df[\"CRA\"], errors=\"ignore\")\n",
        "    df[\"COD_FORMA_EVASAO\"] = df[\"COD_FORMA_EVASAO\"].astype(int)\n",
        "\n",
        "\n",
        "def group_graduated_and_dropped(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Classifies students into groups based on their dropout reason.\n",
        "    \"\"\"\n",
        "    conditions = [\n",
        "        (df[\"COD_FORMA_EVASAO\"] == 4), #Formado\n",
        "        (df[\"COD_FORMA_EVASAO\"] == 1), #Sem evasão\n",
        "        #(Evadidos) Desistência, Jubilado, Desligamento por Abandono, Transferido, Reopção de Curso, Desligamento: Descumpriu Plano de Estudos, Desligamento: Resolução 68/2017-CEPE, Reopção de curso\n",
        "        (df[\"COD_FORMA_EVASAO\"] == 51) | (df[\"COD_FORMA_EVASAO\"] == 10) | (df[\"COD_FORMA_EVASAO\"] == 9) | (df[\"COD_FORMA_EVASAO\"] == 2) | (df[\"COD_FORMA_EVASAO\"] == 20) | (df[\"COD_FORMA_EVASAO\"] == 7) | (df[\"COD_FORMA_EVASAO\"] == 13) | (df[\"COD_FORMA_EVASAO\"] == 53) | (df[\"COD_FORMA_EVASAO\"] == 100),\n",
        "        #(outros) Falecimento, Não Informado, Desligamento por mandado judicial, Adaptação Curricular, Nulidade da matrícula - ato administrativo\n",
        "    ]\n",
        "    values = [\"Formado\", \"Cursando\", \"Evadido\"] #formado, sem evasão, evadido, outros\n",
        "    df[\"GRUP_FORMA_EVASAO\"] = np.select(conditions, values, default=\"Outro\")\n",
        "\n",
        "\n",
        "def group_quotas(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Classifies students as 'Sim' or 'Nao' based on their quota status.\n",
        "    \"\"\"\n",
        "    df[\"COTISTA\"] = df[\"COTISTA\"].map({\"S\": \"Sim\", \"N\": \"Nao\"})\n",
        "\n",
        "\n",
        "def group_entry_mode(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Classifies students based on their entry mode (Vestibular, Sisu, or Other).\n",
        "    \"\"\"\n",
        "    conditions = [\n",
        "        (df[\"FORMA_INGRESSO\"] == \"Vestibular\"),\n",
        "        (df[\"FORMA_INGRESSO\"] == \"Sisu\")\n",
        "    ]\n",
        "    values = [\"Vestibular\", \"Sisu\"]\n",
        "    df[\"FORMA_INGRESSO\"] = np.select(conditions, values, default=\"Outro\")\n",
        "\n",
        "\n",
        "def group_course_status(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Groups course status into 'Aprovado' (Approved) or 'Reprovado' (Failed).\n",
        "    \"\"\"\n",
        "    conditions = [\n",
        "        #(aprovado) Aprovado, Aproveitamento de Estudos, Aprovado sem Nota, Amparo Legal, Dispensa com nota, Dispensa sem nota\n",
        "        (df[\"SITUACAO_ITEM\"] == 1) | (df[\"SITUACAO_ITEM\"] == 11) | (df[\"SITUACAO_ITEM\"] == 8) | (df[\"SITUACAO_ITEM\"] == 19) | (df[\"SITUACAO_ITEM\"] == 7)  | (df[\"SITUACAO_ITEM\"] == 4),\n",
        "        #(reprovado) Reprovado por Freqüência, Reprovado por Nota, Trancamento de Curso , Disciplina sem Oferta, Cancelada\n",
        "        (df[\"SITUACAO_ITEM\"] == 3) | (df[\"SITUACAO_ITEM\"] == 2) | (df[\"SITUACAO_ITEM\"] == 12) | (df[\"SITUACAO_ITEM\"] == 13) | (df[\"SITUACAO_ITEM\"] == 5)\n",
        "    ]\n",
        "    values = [\"Aprovado\", \"Reprovado\"] #aprovado, reprovado\n",
        "    df[\"GRUP_SITUACAO\"] = np.select(conditions, values, default=\"Outro\")\n",
        "\n",
        "\n",
        "def group_failure_status(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Classifies students based on whether they have failed at least one course.\n",
        "    \"\"\"\n",
        "    conditions = [\n",
        "        #(aprovado)\n",
        "        (df[\"GRUP_SITUACAO\"] == \"Aprovado\"),\n",
        "        #(reprovado) em pelo menos 1\n",
        "        (df[\"GRUP_SITUACAO\"] == \"Reprovado\")\n",
        "    ]\n",
        "    values = [\"Não Reprovado\", \"Reprovado\"] #não reprovado, reprovado\n",
        "    df[\"REPROVOU\"] = np.select(conditions, values, default=\"Outro\")\n",
        "\n",
        "\n",
        "def group_failures_due_to_attendance(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Flags students who failed at least one course due to attendance issues.\n",
        "    \"\"\"\n",
        "    conditions = [\n",
        "        #(aprovado)\n",
        "        (df[\"SITUACAO_ITEM\"] != 3),\n",
        "        #(reprovado) em pelo menos 1\n",
        "        (df[\"SITUACAO_ITEM\"] == 3)\n",
        "    ]\n",
        "    values = [\"Não\", \"Sim\"] #não reprovado, reprovado\n",
        "    df[\"REPROVOU_FREQUENCIA\"] = np.select(conditions, values, default=\"Outro\")\n",
        "\n",
        "\n",
        "def group_departments(df: pd.DataFrame) -> tuple[list, list]:\n",
        "    \"\"\"\n",
        "    Groups departments based on course codes and determines top departments for analysis.\n",
        "    \"\"\"\n",
        "    df[\"DEPARTAMENTO\"] = df[\"COD_DISCIPLINA\"].str[:3]\n",
        "    if MODO == \"treinar_e_prever\":\n",
        "        top_departments = list(df[\"DEPARTAMENTO\"].value_counts().nlargest(QNT_MAX_DEPARTMENTS).index)\n",
        "    elif MODO == \"apenas_prever\":\n",
        "        with open(JSON_NAME, \"r\") as f:\n",
        "            config_features = json.load(f)\n",
        "        departments = config_features[\"departments\"]\n",
        "        if departments:\n",
        "            top_departments = [dep[-3:] for dep in departments]\n",
        "    df[\"DEPARTAMENTO\"] = df[\"DEPARTAMENTO\"].where(df[\"DEPARTAMENTO\"].isin(top_departments), np.nan)\n",
        "    top_departments_columns = [f\"REPROVADA_POR_CURSADA_{department}\" for department in top_departments]\n",
        "    return top_departments, top_departments_columns\n",
        "\n",
        "\n",
        "### Data Aggregation and Normalization\n",
        "def check_failure(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Checks and flags students who have failed at least one course.\"\"\"\n",
        "    if df[df[\"GRUP_SITUACAO\"] == \"Reprovado\"].empty:\n",
        "        df[\"REPROVOU\"] = \"Nunca\"\n",
        "    else:\n",
        "        df[\"REPROVOU\"] = \"Sim\"\n",
        "    if df[df[\"REPROVOU_FREQUENCIA\"] == \"Sim\"].empty:\n",
        "        df[\"REPROVOU_FREQUENCIA\"] = \"Nunca\"\n",
        "    else:\n",
        "        df[\"REPROVOU_FREQUENCIA\"] = \"Sim\"\n",
        "\n",
        "\n",
        "def calculate_graduation_time(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculates the graduation time for a student based on enrollment and dropout years.\"\"\"\n",
        "    df_resetado = df.reset_index(drop=True)\n",
        "    max_ano_index = df_resetado[\"ANO\"].idxmax()\n",
        "    min_ano_index = df_resetado[\"ANO\"].idxmin()\n",
        "    if df_resetado.loc[min_ano_index, \"ANO\"] == 0:\n",
        "        df_resetado = df_resetado.drop(df_resetado[df_resetado[\"ANO\"] == 0].index).reset_index(drop=True)\n",
        "        min_ano_index = df_resetado[\"ANO\"].idxmin()\n",
        "    sigla_semestre_min_ano = df_resetado.loc[min_ano_index, \"SIGLA_SEMESTRE\"]\n",
        "    sigla_semestre_max_ano = df_resetado.loc[max_ano_index, \"SIGLA_SEMESTRE\"]\n",
        "\n",
        "    if pd.isna(df_resetado[\"ANO_EVASAO\"].values[0]) or not df_resetado[\"ANO_EVASAO\"].values[0] or df_resetado[\"ANO_EVASAO\"].values[0] == 0:\n",
        "        max_ano = df_resetado[\"ANO\"].max()\n",
        "    else:\n",
        "        max_ano = df_resetado[\"ANO\"].max() if (df_resetado[\"ANO\"].max() >= df_resetado[\"ANO_EVASAO\"].values[0]) else (df_resetado[\"ANO_EVASAO\"].values[0])\n",
        "    if max_ano and not np.isnan(max_ano) and max_ano != 0:\n",
        "        df_resetado[\"ANO_EVASAO\"] = max_ano\n",
        "    min_ano = df_resetado[\"ANO_INGRESSO\"].values[0] #if (df_resetado[\"ANO\"].min() >= df_resetado[\"ANO_INGRESSO\"].values[0]) else (df_resetado[\"ANO\"].min())\n",
        "    min_ano = df_resetado[\"ANO\"].min() if ((max_ano - min_ano) < 0) else (df_resetado[\"ANO_INGRESSO\"].values[0])\n",
        "    if min_ano and not np.isnan(min_ano) and min_ano != 0:\n",
        "        df_resetado[\"ANO_INGRESSO\"] = min_ano\n",
        "\n",
        "    df_resetado[\"SEMESTRE_INGRESSO\"] = sigla_semestre_min_ano\n",
        "    if sigla_semestre_min_ano == 1 and sigla_semestre_max_ano == 2:\n",
        "        df_resetado[\"TEMPO_GRADUACAO\"] = (max_ano - min_ano) + 1\n",
        "    elif (sigla_semestre_min_ano == 1 and sigla_semestre_max_ano == 1) or (sigla_semestre_min_ano == 2 and sigla_semestre_max_ano == 2):\n",
        "        df_resetado[\"TEMPO_GRADUACAO\"] = (max_ano - min_ano) + 0.5\n",
        "    else:\n",
        "        df_resetado[\"TEMPO_GRADUACAO\"] = (max_ano - min_ano)\n",
        "    return df_resetado\n",
        "\n",
        "\n",
        "def calculate_graduation_time_aux(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate auxiliary graduation time based on the year of entry and semester.\n",
        "    \"\"\"\n",
        "    df_list = []\n",
        "    ano_groups = df.groupby(\"ANO\")\n",
        "    for ano, ano_data in ano_groups:\n",
        "        df_resetado = ano_data\n",
        "        min_ano = df_resetado[\"ANO_INGRESSO\"].values[0]\n",
        "\n",
        "        semestre_groups = df_resetado.groupby(\"SIGLA_SEMESTRE\")\n",
        "        for semestre, semestre_data in semestre_groups:\n",
        "            df_semestre = semestre_data\n",
        "            sigla_semestre_min = df_semestre[\"SEMESTRE_INGRESSO\"].values[0]\n",
        "            if sigla_semestre_min == 1 and semestre == 2:\n",
        "                df_semestre[\"TEMPO_GRADUACAO_AUX\"] = (ano - min_ano) + 1\n",
        "            elif (sigla_semestre_min == 1 and semestre == 1) or (sigla_semestre_min == 2 and semestre == 2):\n",
        "                df_semestre[\"TEMPO_GRADUACAO_AUX\"] = (ano - min_ano) + 0.5\n",
        "            else:\n",
        "                df_semestre[\"TEMPO_GRADUACAO_AUX\"] = (ano - min_ano)\n",
        "            df_list.append(df_semestre)\n",
        "        df_final = pd.concat(df_list)\n",
        "    return df_final\n",
        "\n",
        "\n",
        "def calculate_approval_rejection_count(df: pd.DataFrame) -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Calculates the count of approved and rejected students.\n",
        "    \"\"\"\n",
        "    qnt_aprovacao = df[df[\"GRUP_SITUACAO\"] == \"Aprovado\"].count()[0]\n",
        "    qnt_reprovacao = df[df[\"GRUP_SITUACAO\"] == \"Reprovado\"].count()[0]\n",
        "    return qnt_aprovacao, qnt_reprovacao\n",
        "\n",
        "\n",
        "def calculate_var_metrics(df: pd.DataFrame, var: str, approval_count: int, reproval_count: int) -> None:\n",
        "    \"\"\"\n",
        "    Calculates and adds the approval and reproval metrics for a specific var.\n",
        "    \"\"\"\n",
        "    total_courses = approval_count + reproval_count\n",
        "    if total_courses == 0:\n",
        "        df[f\"REPROVADA_POR_CURSADA_{var}\"] = np.nan\n",
        "    else:\n",
        "        df[f\"REPROVADA_POR_CURSADA_{var}\"] = reproval_count/total_courses\n",
        "\n",
        "\n",
        "def separate_approvals_rejections_years(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Separates the dataframe by graduation year and calculates approval and rejection metrics for each year.\n",
        "    \"\"\"\n",
        "    df_aux = calculate_graduation_time_aux(df)\n",
        "    df[\"TEMPO_GRADUACAO_AUX\"] = df_aux[\"TEMPO_GRADUACAO_AUX\"]\n",
        "    df_ano1 = df[df[\"TEMPO_GRADUACAO_AUX\"] <= 1.0]\n",
        "    # df_ano2 = df[df[\"TEMPO_GRADUACAO_AUX\"] > 1.0]\n",
        "    # df_ano2 = df_ano2[df_ano2[\"TEMPO_GRADUACAO_AUX\"] <= 2.0]\n",
        "    # df_ano3 = df[df[\"TEMPO_GRADUACAO_AUX\"] > 2.0]\n",
        "    # df_ano3 = df_ano3[df_ano3[\"TEMPO_GRADUACAO_AUX\"] <= 3.0]\n",
        "    # df_ano4 = df[df[\"TEMPO_GRADUACAO_AUX\"] > 3.0]\n",
        "    # df_ano4 = df_ano4[df_ano4[\"TEMPO_GRADUACAO_AUX\"] <= 4.0]\n",
        "    # df_ano5 = df[df[\"TEMPO_GRADUACAO_AUX\"] > 4.0]\n",
        "    # df_ano5 = df_ano5[df_ano5[\"TEMPO_GRADUACAO_AUX\"] <= 5.0]\n",
        "    # df_ano6 = df[df[\"TEMPO_GRADUACAO_AUX\"] > 5.0]\n",
        "\n",
        "    qnt_aprovacao, qnt_reprovacao = calculate_approval_rejection_count(df_ano1)\n",
        "    calculate_var_metrics(df, \"ANO1\", qnt_aprovacao, qnt_reprovacao)\n",
        "    # qnt_aprovacao, qnt_reprovacao = calculate_approval_rejection_count(df_ano2)\n",
        "    # calculate_var_metrics(df, \"ANO2\", qnt_aprovacao, qnt_reprovacao)\n",
        "    # qnt_aprovacao, qnt_reprovacao = calculate_approval_rejection_count(df_ano3)\n",
        "    # calculate_var_metrics(df, \"ANO3\", qnt_aprovacao, qnt_reprovacao)\n",
        "    # qnt_aprovacao, qnt_reprovacao = calculate_approval_rejection_count(df_ano4)\n",
        "    # calculate_var_metrics(df, \"ANO4\", qnt_aprovacao, qnt_reprovacao)\n",
        "    # qnt_aprovacao, qnt_reprovacao = calculate_approval_rejection_count(df_ano5)\n",
        "    # calculate_var_metrics(df, \"ANO5\", qnt_aprovacao, qnt_reprovacao)\n",
        "    # qnt_aprovacao, qnt_reprovacao = calculate_approval_rejection_count(df_ano6)\n",
        "    # calculate_var_metrics(df, \"ANO>5\", qnt_aprovacao, qnt_reprovacao)\n",
        "\n",
        "\n",
        "def separate_approvals_rejections_departments(df: pd.DataFrame, top_departments: list) -> None:\n",
        "    \"\"\"\n",
        "    Separates the dataframe by department and calculates approval and rejection metrics for each department.\n",
        "    \"\"\"\n",
        "    for department in top_departments:\n",
        "        df[f\"REPROVADA_POR_CURSADA_{department}\"] = np.nan\n",
        "    depart_groups = df.groupby(\"DEPARTAMENTO\")\n",
        "    for depart, depart_data in depart_groups:\n",
        "        qnt_aprovacao, qnt_reprovacao = calculate_approval_rejection_count(depart_data)\n",
        "        calculate_var_metrics(df, str(depart), qnt_aprovacao, qnt_reprovacao)\n",
        "\n",
        "\n",
        "def unify_students(df_list: list, top_departments_columns: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Unifies a list of dataframes by selecting the row with the maximum 'ANO' for each dataframe\n",
        "    and concatenating them into a single dataframe.\n",
        "    \"\"\"\n",
        "    columns = [\n",
        "        \"CRA\",\n",
        "        \"TEMPO_GRADUACAO\",\n",
        "        \"REPROVADA_POR_CURSADA_ANO1\",\n",
        "        \"REPROVOU\",\n",
        "        \"REPROVOU_FREQUENCIA\",\n",
        "        \"GRUP_FORMA_EVASAO\"\n",
        "    ]\n",
        "    columns = MAIN_COLUMNS + columns\n",
        "    columns.extend(top_departments_columns)\n",
        "    df_unificado = df_list[0].head(0).copy()\n",
        "    for df_index in df_list:\n",
        "        df_resetado = df_index.reset_index(drop=True)\n",
        "        df_unificado.loc[len(df_unificado)] = list(df_resetado.iloc[df_resetado[\"ANO\"].idxmax()])\n",
        "    df_unificado = df_unificado[columns]\n",
        "    return df_unificado\n",
        "\n",
        "\n",
        "def separate_eviction_types(df: pd.DataFrame) -> tuple:\n",
        "    \"\"\"\n",
        "    Separates the dataset into different categories of \"evaded\" students based on their eviction reason.\n",
        "    \"\"\"\n",
        "    df_abandono = df[df[\"COD_FORMA_EVASAO\"] == 51] #Abandono\n",
        "    df_tres_reprovacoes = df[df[\"COD_FORMA_EVASAO\"] == 10] #Três reprovações\n",
        "    df_transferido = df[df[\"COD_FORMA_EVASAO\"] == 2] #Transferido\n",
        "    df_desistencia = df[df[\"COD_FORMA_EVASAO\"] == 9] #Desistência\n",
        "    df_jubilado = df[df[\"COD_FORMA_EVASAO\"] == 7] #Jubilado\n",
        "    df_reopcao1 = df[df[\"COD_FORMA_EVASAO\"] == 20] #Reopcao 20\n",
        "    df_reopcao2 = df[df[\"COD_FORMA_EVASAO\"] == 100] #Reopcao 100\n",
        "    df_reopcao= pd.concat([df_reopcao1, df_reopcao2])\n",
        "    df_descumpriu_plano_estudos = df[df[\"COD_FORMA_EVASAO\"] == 13] #Descompriu plano de estudos\n",
        "    df_falecimento = df[df[\"COD_FORMA_EVASAO\"] == 3] #Falecimento\n",
        "    df_nao_informado = df[df[\"COD_FORMA_EVASAO\"] == 99] #Não informado\n",
        "    df_resolucao_CEPE = df[df[\"COD_FORMA_EVASAO\"] == 53] #Resolução CEPE\n",
        "    df_mandado_judicial = df[df[\"COD_FORMA_EVASAO\"] == 300] #Mandado judicial\n",
        "    df_adaptacao_curricular = df[df[\"COD_FORMA_EVASAO\"] == 31] #Adaptacao curricular\n",
        "    df_nulidade_matricula = df[df[\"COD_FORMA_EVASAO\"] == 400] #Nulidade da matricula\n",
        "    return (\n",
        "        df_abandono,\n",
        "        df_tres_reprovacoes,\n",
        "        df_transferido,\n",
        "        df_desistencia,\n",
        "        df_jubilado,\n",
        "        df_reopcao,\n",
        "        df_descumpriu_plano_estudos,\n",
        "        df_falecimento,\n",
        "        df_nao_informado,\n",
        "        df_resolucao_CEPE,\n",
        "        df_mandado_judicial,\n",
        "        df_adaptacao_curricular,\n",
        "        df_nulidade_matricula\n",
        "    )\n",
        "\n",
        "\n",
        "def separate_graduated_and_dropped_out_groups(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Separates the dataset into 'graduated', 'dropped out', and 'no dropout' groups.\n",
        "    \"\"\"\n",
        "    df_graduated = df[df['COD_FORMA_EVASAO'] == 4] #Fomado\n",
        "    df_enrolled = df[df['COD_FORMA_EVASAO'] == 1] #Sem evasão\n",
        "    (\n",
        "        df_abandono,\n",
        "        df_tres_reprovacoes,\n",
        "        df_transferido,\n",
        "        df_desistencia,\n",
        "        df_jubilado,\n",
        "        df_reopcao,\n",
        "        df_descumpriu_plano_estudos,\n",
        "        df_falecimento,\n",
        "        df_nao_informado,\n",
        "        df_resolucao_CEPE,\n",
        "        df_mandado_judicial,\n",
        "        df_adaptacao_curricular,\n",
        "        df_nulidade_matricula\n",
        "    ) = separate_eviction_types(df)\n",
        "    df_dropped_out = pd.concat([\n",
        "        df_abandono,\n",
        "        df_tres_reprovacoes,\n",
        "        df_transferido,\n",
        "        df_desistencia,\n",
        "        df_jubilado,\n",
        "        df_reopcao,\n",
        "        df_descumpriu_plano_estudos,\n",
        "        df_resolucao_CEPE\n",
        "    ])\n",
        "    return df_graduated, df_dropped_out, df_enrolled\n",
        "\n",
        "\n",
        "def separate_grup_forma_evasao(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Separate a DataFrame into three groups based on student status.\n",
        "    \"\"\"\n",
        "    df_graduated = df[df[\"GRUP_FORMA_EVASAO_Formado\"]]\n",
        "    df_dropped_out = df[df[\"GRUP_FORMA_EVASAO_Evadido\"]]\n",
        "    df_enrolled = df[df[\"GRUP_FORMA_EVASAO_Cursando\"]]\n",
        "    return df_graduated, df_dropped_out, df_enrolled\n",
        "\n",
        "\n",
        "def convert_category_types(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Convert specific DataFrame columns to categorical data type.\n",
        "    \"\"\"\n",
        "    df[\"GRUP_FORMA_EVASAO\"] = df[\"GRUP_FORMA_EVASAO\"].astype(\"category\")\n",
        "    df[\"REPROVOU\"] = df[\"REPROVOU\"].astype(\"category\")\n",
        "    df[\"REPROVOU_FREQUENCIA\"] = df[\"REPROVOU_FREQUENCIA\"].astype(\"category\")\n",
        "\n",
        "\n",
        "# Model Training, Tuning, and Predictions\n",
        "def get_dummies_custom(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converts categorical columns into dummy variables and drops the original columns.\n",
        "    \"\"\"\n",
        "    columns_to_get_dummies = df.select_dtypes(include=[\"category\"]).columns.tolist()\n",
        "    for col in columns_to_get_dummies:\n",
        "        df_aux = pd.get_dummies(df[col], prefix=col)\n",
        "        df = pd.concat([df, df_aux], axis=1).drop(col, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_cols_X(df: pd.DataFrame, top_departments_columns: list) -> tuple[list, list]:\n",
        "    \"\"\"\n",
        "    Selects columns for the feature matrix X.\n",
        "    \"\"\"\n",
        "    columns_null_percent = []\n",
        "    null_percent = df[top_departments_columns].isnull().mean() * 100\n",
        "    # print(null_percent)\n",
        "    columns_null_percent = list(null_percent[null_percent < MAX_NULL_PERCENT].index)\n",
        "    print(\"\\nColunas de departamentos selecionas para o modelo:\")\n",
        "    if len(columns_null_percent) == 0:\n",
        "        print(\"Nenhuma coluna de departamento foi selecionada para o modelo.\")\n",
        "    else:\n",
        "        print(columns_null_percent)\n",
        "    col_X = COL_X + columns_null_percent\n",
        "    return col_X, columns_null_percent\n",
        "\n",
        "\n",
        "def create_pipeline(model) -> Pipeline:\n",
        "    \"\"\"\n",
        "    Creates a machine learning pipeline with a transformer and a model.\n",
        "    \"\"\"\n",
        "    pipeline = Pipeline([\n",
        "        (\"transformer\", StandardScaler()),\n",
        "        (\"estimator\", model)\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "def run_cross_validation(X: pd.DataFrame, y: pd.Series, param_grid: Dict[str, list], model) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs cross-validation with hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    rkf = RepeatedStratifiedKFold(n_splits=5, n_repeats=6, random_state=11)\n",
        "    pipeline = create_pipeline(model)\n",
        "\n",
        "    gs = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=\"accuracy\", cv=4)\n",
        "    cv_results = cross_validate(gs, X, y, cv=rkf, return_estimator=True, scoring=\"accuracy\")\n",
        "    return cv_results\n",
        "\n",
        "\n",
        "def compute_metrics(cv_results: Dict[str, Any]) -> list:\n",
        "    \"\"\"\n",
        "    Computes and displays model performance metrics.\n",
        "    \"\"\"\n",
        "    scores = cv_results[\"test_score\"]\n",
        "    mean, std = scores.mean(), scores.std()\n",
        "    print(\"\\nMean Accuracy: {:.4f} | Standard Deviation: {:.4f}\".format(mean, std))\n",
        "    return scores\n",
        "\n",
        "\n",
        "def extract_best_hyperparameters(cv_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extracts the most frequent best hyperparameters across cross-validation folds.\n",
        "    \"\"\"\n",
        "    best_params_list = [frozenset(estimator.best_params_.items()) for estimator in cv_results[\"estimator\"]]\n",
        "    param_counts = Counter(best_params_list)\n",
        "    best_hyperparams = param_counts.most_common(1)[0][0]\n",
        "    return dict(best_hyperparams)\n",
        "\n",
        "\n",
        "def compute_feature_importances(cv_results: Dict[str, Any], X: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Computes and plots feature importance.\n",
        "    \"\"\"\n",
        "    feature_importances = [\n",
        "        estimator.best_estimator_.named_steps[\"estimator\"].feature_importances_\n",
        "        for estimator in cv_results[\"estimator\"]\n",
        "    ]\n",
        "\n",
        "    mean_feature_importances = np.mean(feature_importances, axis=0)\n",
        "    mean_feature_importances /= np.sum(mean_feature_importances)\n",
        "\n",
        "    df_feature_importances = pd.DataFrame({\n",
        "        \"Feature\": X.columns,\n",
        "        \"Importance\": mean_feature_importances\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    blue_palette = [\n",
        "        \"#033053\",  # Deep Ocean Blue\n",
        "        \"#05427B\",  # Navy Blue\n",
        "        \"#05427B\",  # Dark Sapphire (Given)\n",
        "        \"#08558F\",  # Steel Blue\n",
        "        \"#0E65AD\",  # Royal Blue (Given)\n",
        "        \"#2A79C2\",  # Azure Blue\n",
        "        \"#4590D4\",  # Sky Blue\n",
        "        \"#64A8E2\",  # Soft Blue\n",
        "        \"#8BC0EC\",  # Pastel Blue\n",
        "        \"#B4D9F4\",  # Icy Blue\n",
        "        \"#B4D9F4\"   # Frost Blue\n",
        "    ]\n",
        "    custom_palette = sns.color_palette(blue_palette)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=\"Importance\", y=\"Feature\", data=df_feature_importances, palette=custom_palette)\n",
        "    plt.title(\"Feature Importances (Averaged Over Nested CV Folds)\", size=16)\n",
        "    plt.xlabel(\"Importance\", size=14)\n",
        "    plt.ylabel(\"Feature\", size=14)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def clean_hyperparameters(best_hyperparams_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Removes the 'estimator__' prefix from hyperparameters.\n",
        "    \"\"\"\n",
        "    return {k.replace(\"estimator__\", \"\"): v for k, v in best_hyperparams_dict.items()}\n",
        "\n",
        "\n",
        "def train_model_and_save(X: pd.DataFrame, y: pd.Series, model_name: str, model) -> Pipeline:\n",
        "    \"\"\"\n",
        "    Trains the final model using the provided data and saves the trained model as a pickle file in the fixed directory.\n",
        "    \"\"\"\n",
        "    final_model = create_pipeline(model)\n",
        "    final_model.fit(X, y)\n",
        "    with open(model_name, \"wb\") as f:\n",
        "        pickle.dump(final_model, f)\n",
        "    return final_model\n",
        "\n",
        "def get_folder_id(folder_url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the folder ID from a Google Drive folder URL.\n",
        "    \"\"\"\n",
        "    folder_id = folder_url.split(\"/folders/\")[-1]\n",
        "    folder_id = folder_id.split(\"?\")[0]\n",
        "    return folder_id\n",
        "\n",
        "\n",
        "def upload_drive(name: str, folder_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Upload a file to Google Drive.\n",
        "    \"\"\"\n",
        "    mtype, _ = guess_type(name)\n",
        "    metadata = {\n",
        "        \"name\": name,\n",
        "        \"parents\": [folder_id]\n",
        "    }\n",
        "    media = MediaFileUpload(f\"{name}\", mimetype=mtype)\n",
        "    response = (\n",
        "        service_drive.files()\n",
        "        .create(\n",
        "            body=metadata,\n",
        "            media_body=media,\n",
        "            fields=\"id\",\n",
        "            supportsAllDrives=True,\n",
        "        )\n",
        "        .execute()\n",
        "    )\n",
        "\n",
        "\n",
        "def save_config_features(folder_id: str, model_departments_columns: list) -> None:\n",
        "    \"\"\"\n",
        "    Save configuration features to a JSON file and upload it to Google Drive.\n",
        "    \"\"\"\n",
        "    config_features = {\n",
        "        \"departments\": model_departments_columns\n",
        "    }\n",
        "    with open(JSON_NAME, \"w\") as f:\n",
        "        json.dump(config_features, f, indent=4)\n",
        "    upload_drive(JSON_NAME, folder_id)\n",
        "\n",
        "\n",
        "def load_file_info_drive(folder_id: str) -> tuple[dict, dict]:\n",
        "    \"\"\"\n",
        "    Load file information for model and JSON files from Google Drive.\n",
        "    \"\"\"\n",
        "    file_info_json = {}\n",
        "    files_model = []\n",
        "    files_json = []\n",
        "    files = (\n",
        "        service_drive.files()\n",
        "        .list(\n",
        "            fields=\"files(id,name,modifiedTime)\",\n",
        "            q=f\"'{folder_id}' in parents\",\n",
        "            supportsAllDrives=True,\n",
        "            includeItemsFromAllDrives=True,\n",
        "        )\n",
        "        .execute()[\"files\"]\n",
        "    )\n",
        "    for file_dict in files:\n",
        "        if file_dict[\"name\"] == MODEL_NAME:\n",
        "            files_model.append(\n",
        "                {\n",
        "                    \"name\": file_dict[\"name\"],\n",
        "                    \"id\": file_dict[\"id\"],\n",
        "                    \"modified_time\": file_dict[\"modifiedTime\"]\n",
        "                }\n",
        "            )\n",
        "        elif file_dict[\"name\"] == JSON_NAME:\n",
        "            files_json.append(\n",
        "                {\n",
        "                    \"name\": file_dict[\"name\"],\n",
        "                    \"id\": file_dict[\"id\"],\n",
        "                    \"modified_time\": file_dict[\"modifiedTime\"]\n",
        "                }\n",
        "            )\n",
        "    if len(files_model) == 0:\n",
        "        raise FileNotFoundError(f\"Arquivo '{MODEL_NAME}' não encontrado no Google Drive.\")\n",
        "    if len(files_json) == 0:\n",
        "        raise FileNotFoundError(f\"Arquivo '{JSON_NAME}' não encontrado no Google Drive.\")\n",
        "    file_info_model = max(files_model, key=lambda x: x[\"modified_time\"])\n",
        "    file_info_json = max(files_json, key=lambda x: x[\"modified_time\"])\n",
        "    return file_info_model, file_info_json\n",
        "\n",
        "\n",
        "def download_file_drive(file_info: dict) -> None:\n",
        "    \"\"\"\n",
        "    Downloads a file from Google Drive using the Drive API.\n",
        "    \"\"\"\n",
        "    request = service_drive.files().get_media(\n",
        "        fileId=file_info[\"id\"], supportsAllDrives=True\n",
        "    )\n",
        "    file = io.FileIO(file_info[\"name\"], \"wb\")\n",
        "    downloader = MediaIoBaseDownload(file, request)\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "\n",
        "\n",
        "def load_model(model_name: str) -> Pipeline:\n",
        "    \"\"\"\n",
        "    Loads a trained model from a pickle file.\n",
        "    \"\"\"\n",
        "    with open(model_name, \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    return model\n",
        "\n",
        "\n",
        "def make_predictions(model: Pipeline, X_new: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Makes predictions on new data and returns a DataFrame with the predictions.\n",
        "    \"\"\"\n",
        "    y_pred_new = model.predict(X_new)\n",
        "    df_y_pred = pd.DataFrame(y_pred_new, columns=[\"y_pred\"])\n",
        "    return df_y_pred\n",
        "\n",
        "\n",
        "def save_predictions(df_data: pd.DataFrame, df_y_pred: pd.DataFrame, filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Merges predictions with original data, selects predefined columns, and saves the results to a excel file.\n",
        "\n",
        "    \"\"\"\n",
        "    df_merged = df_data.join(df_y_pred, how=\"inner\")\n",
        "    df_pred = df_merged[df_merged[\"y_pred\"] == 1]\n",
        "    df_evadidos = df_pred.sort_values(by=[\"ANO_INGRESSO\"], ascending=False).reset_index(drop=True)\n",
        "    df_evadidos = df_evadidos.drop(\"y_pred\", axis=1)\n",
        "    df_evadidos.to_excel(filename, index=False)\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "\n",
        "# MAIN\n",
        "\n",
        "model = None\n",
        "folder_id = get_folder_id(URL_PASTA_DRIVE.strip())\n",
        "\n",
        "# Data Loading\n",
        "df_upload = upload_files()\n",
        "\n",
        "# Data Cleaning, Transformation, and Aggregation\n",
        "df = df_upload.drop_duplicates()\n",
        "df = df.dropna(subset=[\"COD_FORMA_EVASAO\"])\n",
        "convert_types(df)\n",
        "group_graduated_and_dropped(df)\n",
        "group_quotas(df)\n",
        "group_entry_mode(df)\n",
        "group_course_status(df)\n",
        "group_failure_status(df)\n",
        "group_failures_due_to_attendance(df)\n",
        "\n",
        "if MODO == \"apenas_prever\":\n",
        "    file_info_model, file_info_json = load_file_info_drive(folder_id)\n",
        "    download_file_drive(file_info_model)\n",
        "    download_file_drive(file_info_json)\n",
        "top_departments, top_departments_columns = group_departments(df)\n",
        "\n",
        "### Data Aggregation and Normalization\n",
        "df_list = []\n",
        "qnt = 0\n",
        "matr_groups = df.groupby(\"MATR_ALUNO\")\n",
        "for matr, matr_data in matr_groups:\n",
        "    df_aux = matr_data.copy().reset_index(drop=True)\n",
        "    check_failure(df_aux)\n",
        "    df_aux = calculate_graduation_time(df_aux)\n",
        "    separate_approvals_rejections_years(df_aux)\n",
        "    separate_approvals_rejections_departments(df_aux, top_departments)\n",
        "    df_list.append(df_aux)\n",
        "df = unify_students(df_list, top_departments_columns)\n",
        "\n",
        "# Model Training, Tuning, and Predictions\n",
        "convert_category_types(df)\n",
        "df = get_dummies_custom(df)\n",
        "col_X, model_departments_columns = get_cols_X(df, top_departments_columns)\n",
        "df = df.dropna(subset=col_X)\n",
        "df_graduated, df_dropped_out, df_enrolled = separate_grup_forma_evasao(df)\n",
        "df_graduated_dropped_out = pd.concat([df_graduated, df_dropped_out])\n",
        "\n",
        "#### XGBoost\n",
        "df_graduated_dropped_out = df_graduated_dropped_out.reset_index(drop=True)\n",
        "columns = MAIN_COLUMNS + col_X\n",
        "df_enrolled = df_enrolled[columns]\n",
        "X_new = df_enrolled.reset_index(drop=True)\n",
        "\n",
        "if MODO == \"treinar_e_prever\":\n",
        "    X = df_graduated_dropped_out[col_X]\n",
        "    y = df_graduated_dropped_out[col_y]\n",
        "    model = XGBClassifier(random_state=42)\n",
        "    param_grid = {\n",
        "        \"estimator__learning_rate\": [0.1, 0.2, 0.3],\n",
        "        \"estimator__max_depth\": [3, 6, 9],\n",
        "        \"estimator__min_child_weight\": [1, 3, 5]\n",
        "    }\n",
        "    cv_results = run_cross_validation(X, y, param_grid, model)\n",
        "    scores_eng_xgb = compute_metrics(cv_results)\n",
        "    best_hyperparams_dict = extract_best_hyperparameters(cv_results)\n",
        "    print(\"\\nBest Hyperparameters (Most Frequent in Nested CV):\")\n",
        "    print(best_hyperparams_dict)\n",
        "    compute_feature_importances(cv_results, X)\n",
        "\n",
        "    best_hyperparams_clean = clean_hyperparameters(best_hyperparams_dict)\n",
        "    model = XGBClassifier(**best_hyperparams_clean, random_state=42)\n",
        "    final_model = train_model_and_save(X, y, MODEL_NAME, model)\n",
        "    if URL_PASTA_DRIVE.strip():\n",
        "        upload_drive(MODEL_NAME, folder_id)\n",
        "        save_config_features(folder_id, model_departments_columns)\n",
        "    predictions = make_predictions(final_model, X_new[col_X])\n",
        "    save_predictions(X_new, predictions, OUTPUT_FILENAME)\n",
        "\n",
        "elif MODO == \"apenas_prever\":\n",
        "    loaded_model = load_model(MODEL_NAME)\n",
        "    predictions = make_predictions(loaded_model, X_new[col_X])\n",
        "    save_predictions(X_new, predictions, OUTPUT_FILENAME)\n"
      ]
    }
  ]
}